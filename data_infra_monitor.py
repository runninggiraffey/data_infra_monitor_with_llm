#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM ê¸°ë°˜ ë°ì´í„° ì¸í”„ë¼ ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤
"""

import os
import json
import re
from datetime import datetime, timedelta
from typing import TypedDict, List, Dict, Optional, Literal, Annotated
from operator import add

# LangChain ê´€ë ¨ imports
from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# LangGraph ê´€ë ¨ imports
from langgraph.graph import StateGraph, START, END
from langgraph.types import Send
from langgraph.checkpoint.memory import InMemorySaver

# Gradio
import gradio as gr

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
from dotenv import load_dotenv
load_dotenv()

# # Langfuse ì¶”ì  í™œì„±í™”
# os.environ["LANGFUSE_PUBLIC_KEY"] = os.getenv("LANGFUSE_PUBLIC_KEY", "")
# os.environ["LANGFUSE_SECRET_KEY"] = os.getenv("LANGFUSE_SECRET_KEY", "")
# os.environ["LANGFUSE_HOST"] = os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com")
# os.environ["LANGCHAIN_TRACING_V2"] = "false"
# os.environ["LANGCHAIN_ENDPOINT"] = ""

# ============================================================================
# 1. ìƒíƒœ ì •ì˜
# ============================================================================

class InfrastructureState(TypedDict):
    """ì¸í”„ë¼ ëª¨ë‹ˆí„°ë§ ìƒíƒœ"""
    # ì…ë ¥ ë°ì´í„°
    user_query: str                    # ì‚¬ìš©ì ì§ˆë¬¸
    alert_data: Optional[Dict]         # ì•ŒëŒ ë°ì´í„°
    date_range: Optional[str]          # ë¶„ì„ ê¸°ê°„
    
    # ë¼ìš°íŒ… ì •ë³´
    route: Optional[str]               # ë¼ìš°íŒ… ê²°ê³¼
    
    # ìˆ˜ì§‘ëœ ë°ì´í„°
    hadoop_metrics: List[Dict]         # í•˜ë‘¡ ë©”íŠ¸ë¦­
    presto_metrics: List[Dict]         # Presto ë©”íŠ¸ë¦­
    jenkins_metrics: List[Dict]        # Jenkins ë©”íŠ¸ë¦­
    grafana_metrics: List[Dict]        # Grafana ë©”íŠ¸ë¦­
    
    # ë¶„ì„ ê²°ê³¼
    daily_report: Optional[str]        # ì¼ì¼ ë¦¬í¬íŠ¸
    alert_analysis: Optional[str]      # ì•ŒëŒ ë¶„ì„ ê²°ê³¼
    spark_jobs: List[Dict]             # Spark ì¡ ì •ë³´
    recommendations: List[str]         # ê¶Œì¥ì‚¬í•­
    
    # ì—ì´ì „íŠ¸ ì‘ë‹µ
    infra_engineer_response: Optional[str]
    data_engineer_response: Optional[str]
    admin_guide_response: Optional[str]
    
    # ìµœì¢… ê²°ê³¼
    final_answer: Optional[str]

# ============================================================================
# 2. ë„êµ¬ ì •ì˜ (Tool Functions)
# ============================================================================

@tool
def get_cluster_metrics() -> Dict:
    """í´ëŸ¬ìŠ¤í„° ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    # ë°ëª¨ìš© ê°€ìƒ ë°ì´í„°
    return {
        "timestamp": datetime.now().isoformat(),
        "cpu_usage": {
            "overall": 75.5,
            "node1": 82.3,
            "node2": 68.7,
            "node3": 79.1
        },
        "memory_usage": {
            "overall": 78.2,
            "node1": 85.6,
            "node2": 72.4,
            "node3": 76.5
        },
        "disk_usage": {
            "overall": 65.8,
            "node1": 71.2,
            "node2": 62.3,
            "node3": 63.9
        },
        "network_usage": {
            "overall": 45.3,
            "node1": 52.1,
            "node2": 41.8,
            "node3": 42.0
        }
    }

@tool
def get_spark_jobs() -> List[Dict]:
    """í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ Spark ì¡ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    # ë°ëª¨ìš© ê°€ìƒ ë°ì´í„°
    return [
        {
            "job_id": "spark_001",
            "duration": "2h 15m",
            "cpu_usage": 85.2,
            "memory_usage": 78.5
        },
        {
            "job_id": "spark_002",
            "duration": "3h 45m",
            "cpu_usage": 92.1,
            "memory_usage": 89.3
        },
        {
            "job_id": "spark_003",
            "duration": "1h 30m",
            "cpu_usage": 45.7,
            "memory_usage": 52.1
        }
    ]

@tool
def kill_spark_job(job_id: str) -> Dict:
    """ì§€ì •ëœ Spark ì¡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."""
    # ë°ëª¨ìš© ê°€ìƒ ì‘ë‹µ
    return {
        "job_id": job_id,
        "action": "KILLED",
        "timestamp": datetime.now().isoformat(),
        "status": "SUCCESS",
        "message": f"Job {job_id} has been successfully terminated."
    }

# ìš´ì˜ ë§¤ë‰´ì–¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
def initialize_operation_manual_db():
    """ìš´ì˜ ë§¤ë‰´ì–¼ì„ Chroma ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œë“œ"""
    from langchain_community.document_loaders import TextLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    
    # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ
    loader = TextLoader("data/operation_manual.txt", encoding="utf-8")
    documents = loader.load()
    
    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    splits = text_splitter.split_documents(documents)
    
    # Chroma ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        collection_name="operation_manual",
        persist_directory="./chroma_db"
    )
    
    return vectorstore

# ì „ì—­ ë³€ìˆ˜ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
operation_manual_vectorstore = None

@tool
def search_operation_manual(query: str) -> List[Document]:
    """ìš´ì˜ ë§¤ë‰´ì–¼ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤."""
    global operation_manual_vectorstore
    
    # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
    if operation_manual_vectorstore is None:
        print("ìš´ì˜ ë§¤ë‰´ì–¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        operation_manual_vectorstore = initialize_operation_manual_db()
    
    # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
    docs = operation_manual_vectorstore.similarity_search(query, k=3)
    
    if docs:
        return docs
    else:
        return [Document(
            page_content="ê´€ë ¨ ìš´ì˜ ë§¤ë‰´ì–¼ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            metadata={"source": "yg_operation_manual.txt"}
        )]

# ìš´ì˜ ì´ë ¥ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
def initialize_operation_history_db():
    """ìš´ì˜ ì´ë ¥ì„ Chroma ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œë“œ"""
    from langchain_community.document_loaders import TextLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    
    # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ
    loader = TextLoader("data/operation_history.txt", encoding="utf-8")
    documents = loader.load()
    
    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    splits = text_splitter.split_documents(documents)
    
    # Chroma ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        collection_name="operation_history",
        persist_directory="./chroma_db"
    )
    
    return vectorstore

# ì „ì—­ ë³€ìˆ˜ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
operation_history_vectorstore = None

@tool
def search_operation_history(query: str) -> List[Document]:
    """ìš´ì˜ ì´ë ¥ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤."""
    global operation_history_vectorstore
    
    # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
    if operation_history_vectorstore is None:
        print("ìš´ì˜ ì´ë ¥ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        operation_history_vectorstore = initialize_operation_history_db()
    
    # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
    docs = operation_history_vectorstore.similarity_search(query, k=3)
    
    if docs:
        return docs
    else:
        return [Document(
            page_content="ê´€ë ¨ ìš´ì˜ ì´ë ¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            metadata={"source": "yg_operation_history.txt"}
        )]

@tool
def search_internet_info(query: str) -> List[Document]:
    """Tavilyë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸í„°ë„·ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        from tavily import TavilyClient
        
        # Tavily í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        tavily_api_key = os.getenv("TAVILY_API_KEY")
        if not tavily_api_key:
            error_msg = "TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¸í„°ë„· ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            print(error_msg)
            return [Document(
                page_content=error_msg,
                metadata={"source": "error", "error_type": "missing_api_key"}
            )]
        
        client = TavilyClient(api_key=tavily_api_key)
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ê°œì„  (ë°ì´í„° ì¸í”„ë¼ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€)
        enhanced_query = f"{query} data infrastructure monitoring best practices"
        
        # Tavily ê²€ìƒ‰ ì‹¤í–‰
        search_result = client.search(
            query=enhanced_query,
            search_depth="advanced",
            max_results=5,
            include_domains=["apache.org", "databricks.com", "cloudera.com", "hortonworks.com", "stackoverflow.com", "medium.com", "towardsdatascience.com"]
        )
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ Document í˜•íƒœë¡œ ë³€í™˜
        documents = []
        for result in search_result.get("results", []):
            content = result.get("content", "")
            url = result.get("url", "")
            title = result.get("title", "")
            
            if content:
                # ì œëª©ê³¼ ë‚´ìš©ì„ ê²°í•©
                full_content = f"ì œëª©: {title}\n\në‚´ìš©: {content}"
                documents.append(Document(
                    page_content=full_content,
                    metadata={
                        "source": "tavily_search",
                        "url": url,
                        "title": title
                    }
                ))
        
        if documents:
            return documents
        else:
            error_msg = "Tavily ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì¸í„°ë„· ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            print(error_msg)
            return [Document(
                page_content=error_msg,
                metadata={"source": "error", "error_type": "no_search_results"}
            )]
            
    except Exception as e:
        error_msg = f"Tavily ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì¸í„°ë„· ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        print(error_msg)
        return [Document(
            page_content=error_msg,
            metadata={"source": "error", "error_type": "search_error", "error_details": str(e)}
        )]

# ë„êµ¬ ëª©ë¡
tools = [
    get_cluster_metrics,
    get_spark_jobs,
    kill_spark_job,
    search_operation_manual,
    search_operation_history,
    search_internet_info
]

# ============================================================================
# 3. LLM ëª¨ë¸ ì„¤ì •
# ============================================================================

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
llm_with_tools = llm.bind_tools(tools)

# ============================================================================
# 4. ì—ì´ì „íŠ¸ êµ¬í˜„
# ============================================================================

def infra_engineer_agent(state: InfrastructureState) -> InfrastructureState:
    """ì¸í”„ë¼ ì—”ì§€ë‹ˆì–´ ì—ì´ì „íŠ¸ - í´ëŸ¬ìŠ¤í„° ê°€ë™ìœ¨ ë° ì‹œìŠ¤í…œ í˜„í™© ë‹´ë‹¹"""
    print("--- ì¸í”„ë¼ ì—”ì§€ë‹ˆì–´ ì—ì´ì „íŠ¸ ì‹œì‘ ---")
    
    query = state["user_query"]
    
    # í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸° (Spark ì¡ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ ë³€ê²½)
    ai_msg = llm_with_tools.invoke(query)

    tool_call = ai_msg.tool_calls[0]
    tool_name = tool_call["name"]
    tool_args = tool_call["args"]

    for tool in tools:
        if tool_name == tool.name:
            metrics = tool.invoke(tool_args)
    
    # ë¶„ì„ í”„ë¡¬í”„íŠ¸
    analysis_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë°ì´í„° ì¸í”„ë¼ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„°ì˜ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© í˜„í™©ì„ ë¶„ì„í•˜ê³  ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
        
        ë¶„ì„í•´ì•¼ í•  í•­ëª©:
        1. CPU ì‚¬ìš©ë¥  (ì „ì²´ ë° ê°œë³„ ë…¸ë“œ)
        2. ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (ì „ì²´ ë° ê°œë³„ ë…¸ë“œ)
        3. ë””ìŠ¤í¬ ì‚¬ìš©ë¥  (ì „ì²´ ë° ê°œë³„ ë…¸ë“œ)
        4. ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©ë¥  (ì „ì²´ ë° ê°œë³„ ë…¸ë“œ)
        
        ë‹µë³€ í˜•ì‹:
        - í˜„ì¬ í´ëŸ¬ìŠ¤í„° ìƒíƒœ ìš”ì•½
        - ê° ë…¸ë“œë³„ ìƒì„¸ í˜„í™©
        - ì£¼ì˜ê°€ í•„ìš”í•œ ë¶€ë¶„
        - ê¶Œì¥ì‚¬í•­"""),
        ("human", "ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\ní´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­: {metrics}")
    ])
    
    response = llm.invoke(analysis_prompt.format(
        query=query,
        metrics=json.dumps(metrics, indent=2, ensure_ascii=False)
    ))
    
    return {
        "hadoop_metrics": [metrics],
        "infra_engineer_response": response.content,
        "data_engineer_response": state.get("data_engineer_response", ""),
        "admin_guide_response": state.get("admin_guide_response", "")
    }

def data_engineer_agent(state: InfrastructureState) -> InfrastructureState:
    """ë°ì´í„° ì—”ì§€ë‹ˆì–´ ì—ì´ì „íŠ¸ - Spark ì¡ ê´€ë¦¬ ë‹´ë‹¹"""
    print("--- ë°ì´í„° ì—”ì§€ë‹ˆì–´ ì—ì´ì „íŠ¸ ì‹œì‘ ---")
    
    query = state["user_query"]
    
    # Spark ì¡ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    ai_msg = llm_with_tools.invoke(query)

    tool_call = ai_msg.tool_calls[0]
    tool_name = tool_call["name"]
    tool_args = tool_call["args"]

    for tool in tools:
        if tool_name == tool.name:
            spark_jobs = tool.invoke(tool_args)
    
    # ìƒìœ„ 3ê°œ ì¡ ì¶”ì¶œ (CPU ì‚¬ìš©ë¥  ê¸°ì¤€)
    top_jobs = sorted(spark_jobs, key=lambda x: x["cpu_usage"], reverse=True)[:3]
    
    # ë¶„ì„ í”„ë¡¬í”„íŠ¸
    analysis_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë°ì´í„° ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. Spark ì¡ í˜„í™©ì„ ë¶„ì„í•˜ê³  ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
        
        ë¶„ì„í•´ì•¼ í•  í•­ëª©:
        1. í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ Spark ì¡ ëª©ë¡
        2. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì´ ë†’ì€ ì¡ ì‹ë³„
        3. ìˆ˜í–‰ ì‹œê°„ì´ ê¸´ ì¡ í™•ì¸
        4. ì¡ ì¢…ë£Œ ê¶Œì¥ì‚¬í•­
        
        ë‹µë³€ í˜•ì‹:
        - Spark ì¡ í˜„í™© ìš”ì•½
        - ìƒìœ„ 3ê°œ ì¡ ìƒì„¸ ì •ë³´
        - ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„
        - ì¡ ê´€ë¦¬ ê¶Œì¥ì‚¬í•­"""),
        ("human", "ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\nSpark ì¡ ì •ë³´: {spark_jobs}")
    ])
    
    response = llm.invoke(analysis_prompt.format(
        query=query,
        spark_jobs=json.dumps(top_jobs, indent=2, ensure_ascii=False)
    ))
    
    return {
        "spark_jobs": top_jobs,
        "data_engineer_response": response.content,
        "infra_engineer_response": state.get("infra_engineer_response", ""),
        "admin_guide_response": state.get("admin_guide_response", "")
    }

def admin_guide_agent(state: InfrastructureState) -> InfrastructureState:
    """ê´€ë¦¬ì ê°€ì´ë“œ ì—ì´ì „íŠ¸ - ìš´ì˜ ë§¤ë‰´ì–¼ ë° ì´ë ¥ ê²€ìƒ‰ ë‹´ë‹¹"""
    print("--- ê´€ë¦¬ì ê°€ì´ë“œ ì—ì´ì „íŠ¸ ì‹œì‘ ---")
    
    query = state["user_query"]
    
    # ìš´ì˜ ë§¤ë‰´ì–¼, ìš´ì˜ ì´ë ¥, ì¸í„°ë„· ì •ë³´ ê²€ìƒ‰ì„ ê°ê° tool í˜¸ì¶œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬ (ai_msg ì¤‘ë³µ ë°©ì§€)
    manual_tool_call = llm_with_tools.invoke(f"ìš´ì˜ ë§¤ë‰´ì–¼ì—ì„œ ë‹µì„ ì°¾ì•„ì¤˜: {query}")
    manual_tool_name = manual_tool_call.tool_calls[0]["name"]
    manual_tool_args = manual_tool_call.tool_calls[0]["args"]
    manual_docs = None
    for tool in tools:
        if manual_tool_name == tool.name:
            manual_docs = tool.invoke(manual_tool_args)

    history_tool_call = llm_with_tools.invoke(f"ìš´ì˜ ì´ë ¥ì—ì„œ ë‹µì„ ì°¾ì•„ì¤˜: {query}")
    history_tool_name = history_tool_call.tool_calls[0]["name"]
    history_tool_args = history_tool_call.tool_calls[0]["args"]
    history_docs = None
    for tool in tools:
        if history_tool_name == tool.name:
            history_docs = tool.invoke(history_tool_args)

    internet_tool_call = llm_with_tools.invoke(f"ì¸í„°ë„·ì—ì„œ ë‹µì„ ì°¾ì•„ì¤˜: {query}")
    internet_tool_name = internet_tool_call.tool_calls[0]["name"]
    internet_tool_args = internet_tool_call.tool_calls[0]["args"]
    internet_docs = None
    for tool in tools:
        if internet_tool_name == tool.name:
            internet_docs = tool.invoke(internet_tool_args)
    
    # ë¶„ì„ í”„ë¡¬í”„íŠ¸
    analysis_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë°ì´í„° ì¸í”„ë¼ ê´€ë¦¬ìì…ë‹ˆë‹¤. ìš´ì˜ ë§¤ë‰´ì–¼, ì´ì „ ìš´ì˜ ì´ë ¥, ì¸í„°ë„· ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
        
        ì°¸ê³  ìë£Œ:
        1. ìš´ì˜ ë§¤ë‰´ì–¼: í‘œì¤€ ìš´ì˜ ì ˆì°¨ ë° ê°€ì´ë“œë¼ì¸
        2. ìš´ì˜ ì´ë ¥: ì´ì „ì— ë°œìƒí•œ ìœ ì‚¬ ìƒí™©ê³¼ í•´ê²° ë°©ë²•
        3. ì¸í„°ë„· ì •ë³´: ìµœì‹  ê¸°ìˆ  ì •ë³´ ë° ëª¨ë²” ì‚¬ë¡€
        
        ë‹µë³€ í˜•ì‹:
        - ìš´ì˜ ë§¤ë‰´ì–¼ ê¸°ë°˜ í‘œì¤€ ì ˆì°¨
        - ì´ì „ ì‚¬ë¡€ ë¶„ì„
        - ìµœì‹  ì •ë³´ ë° ê¶Œì¥ì‚¬í•­
        - ì¢…í•©ì ì¸ í•´ê²° ë°©ì•ˆ"""),
        ("human", """ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìš´ì˜ ë§¤ë‰´ì–¼:
{manual_docs}

ìš´ì˜ ì´ë ¥:
{history_docs}

ì¸í„°ë„· ì •ë³´:
{internet_docs}""")
    ])
    
    response = llm.invoke(analysis_prompt.format(
        query=query,
        manual_docs="\n".join([doc.page_content for doc in manual_docs]),
        history_docs="\n".join([doc.page_content for doc in history_docs]),
        internet_docs="\n".join([doc.page_content for doc in internet_docs])
    ))
    
    return {
        "admin_guide_response": response.content,
        "infra_engineer_response": state.get("infra_engineer_response", ""),
        "data_engineer_response": state.get("data_engineer_response", "")
    }

def route_query(state: InfrastructureState) -> InfrastructureState:
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…"""
    print("--- ì§ˆë¬¸ ë¼ìš°íŒ… ë¶„ì„ ---")
    
    query = state["user_query"].lower()
    
    # ì¸í”„ë¼ ê´€ë ¨ í‚¤ì›Œë“œ
    infra_keywords = [
        "í´ëŸ¬ìŠ¤í„°", "cpu", "ë©”ëª¨ë¦¬", "ë””ìŠ¤í¬", "ë„¤íŠ¸ì›Œí¬", "ë¦¬ì†ŒìŠ¤", "ê°€ë™ìœ¨", "ì‚¬ìš©ë¥ ",
        "ì‹œìŠ¤í…œ", "ë…¸ë“œ", "ì„œë²„", "ì„±ëŠ¥", "ë¶€í•˜", "ë¶€ì¡±", "ë†’ìŒ", "ë‚®ìŒ",
        "ëª¨ë‹ˆí„°ë§", "ë©”íŠ¸ë¦­", "í˜„í™©", "ìƒíƒœ", "í™•ì¸", "ì ê²€"
    ]
    
    # ë°ì´í„°/ë°ì´í„°í”Œë«í¼ ê´€ë ¨ í‚¤ì›Œë“œ
    data_keywords = [
        "spark", "hadoop", "hdfs", "yarn", "presto", "hive", "kafka", "flink",
        "ì¡", "job", "íŒŒì´í”„ë¼ì¸", "pipeline", "etl", "ë°ì´í„°", "ì²˜ë¦¬",
        "kill", "ì¢…ë£Œ", "ì‹¤í–‰", "ìŠ¤ì¼€ì¤„", "í", "ë©”ëª¨ë¦¬", "cpu ì‚¬ìš©ë¥ "
    ]
    
    # ìš´ì˜ ë§¤ë‰´ì–¼/ì´ë ¥ ê´€ë ¨ í‚¤ì›Œë“œ
    admin_keywords = [
        "ë§¤ë‰´ì–¼", "ìš´ì˜", "ì ˆì°¨", "ê°€ì´ë“œ", "ë°©ë²•", "ì¡°ì¹˜", "ëŒ€ì‘", "í•´ê²°",
        "ì´ë ¥", "ê²½í—˜", "ì‚¬ë¡€", "ì „ë¡€", "ì´ì „", "ê³¼ê±°", "í•™ìŠµì ",
        "ì–´ë–»ê²Œ", "ë¬´ì—‡ì„", "ì™œ", "ì–¸ì œ", "ì–´ë””ì„œ", "ëˆ„ê°€"
    ]
    
    # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
    infra_score = sum(1 for keyword in infra_keywords if keyword in query)
    data_score = sum(1 for keyword in data_keywords if keyword in query)
    admin_score = sum(1 for keyword in admin_keywords if keyword in query)
    
    print(f"ë¼ìš°íŒ… ì ìˆ˜ - ì¸í”„ë¼: {infra_score}, ë°ì´í„°: {data_score}, ê´€ë¦¬ì: {admin_score}")
    
    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…
    if data_score > infra_score and data_score > admin_score:
        print("â†’ ë°ì´í„° ì—”ì§€ë‹ˆì–´ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…")
        return {"route": "data_engineer"}
    elif admin_score > infra_score and admin_score > data_score:
        print("â†’ ê´€ë¦¬ì ê°€ì´ë“œ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…")
        return {"route": "admin_guide"}
    else:
        print("â†’ ì¸í”„ë¼ ì—”ì§€ë‹ˆì–´ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…")
        return {"route": "infra_engineer"}

def generate_final_answer(state: InfrastructureState) -> InfrastructureState:
    """ìµœì¢… ë‹µë³€ ìƒì„±"""
    print("--- ìµœì¢… ë‹µë³€ ìƒì„± ---")
    
    query = state["user_query"]
    infra_response = state.get("infra_engineer_response", "")
    data_response = state.get("data_engineer_response", "")
    admin_response = state.get("admin_guide_response", "")
    
    # ìµœì¢… ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
    final_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë°ì´í„° ì¸í”„ë¼ ëª¨ë‹ˆí„°ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì²˜ë¦¬ëœ ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
       
        ì¸í”„ë¼ ë° ë°ì´í„° ì—”ì§€ë‹ˆì–´ì— ëŒ€í•œ ë¬¸ì˜ì‚¬í•­ì¸ ê²½ìš° ë‹µë³€ ì–‘ì‹
            1. í˜„ì¬ ìƒí™© ìš”ì•½
            2. ìƒì„¸ ë¶„ì„ ê²°ê³¼
            3. êµ¬ì²´ì ì¸ ì¡°ì¹˜ ë°©ì•ˆ
            4. ì˜ˆë°© ë° ëª¨ë‹ˆí„°ë§ ê¶Œì¥ì‚¬í•­
         
        ê´€ë¦¬ì ê°€ì´ë“œì— ëŒ€í•œ ë¬¸ì˜ì‚¬í•­ì— ëŒ€í•œ ë‹µë³€ êµ¬ì¡° 
            1. ìš´ì˜ ë§¤ë‰´ì–¼ ê¸°ë°˜ í‘œì¤€ ì ˆì°¨
            2. ì´ì „ ì‚¬ë¡€ ë¶„ì„
            3. ìµœì‹  ì •ë³´ ë° ê¶Œì¥ì‚¬í•­
            4. ì¢…í•©ì ì¸ í•´ê²° ë°©ì•ˆ 
         
        ê° ì—ì´ì „íŠ¸ì˜ ì „ë¬¸ ë¶„ì•¼:
        - ì¸í”„ë¼ ì—”ì§€ë‹ˆì–´: ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í˜„í™© ë° í´ëŸ¬ìŠ¤í„° ìƒíƒœ
        - ë°ì´í„° ì—”ì§€ë‹ˆì–´: Spark ì¡ ê´€ë¦¬ ë° ìµœì í™”
        - ê´€ë¦¬ì ê°€ì´ë“œ: ìš´ì˜ ì ˆì°¨ ë° ì´ì „ ì‚¬ë¡€"""),
        ("human", """ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì¸í”„ë¼ ì—”ì§€ë‹ˆì–´ ì‘ë‹µ:
{infra_response}

ë°ì´í„° ì—”ì§€ë‹ˆì–´ ì‘ë‹µ:
{data_response}

ê´€ë¦¬ì ê°€ì´ë“œ ì‘ë‹µ:
{admin_response}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.""")
    ])
    
    response = llm.invoke(final_prompt.format(
        query=query,
        infra_response=infra_response,
        data_response=data_response,
        admin_response=admin_response
    ))
    
    return {
        "final_answer": response.content
    }

# ============================================================================
# 5. LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
# ============================================================================

def create_infrastructure_monitoring_workflow():
    """ì¸í”„ë¼ ëª¨ë‹ˆí„°ë§ ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    
    # ê·¸ë˜í”„ ìƒì„±
    workflow = StateGraph(InfrastructureState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("route", route_query)
    workflow.add_node("infra_engineer", infra_engineer_agent)
    workflow.add_node("data_engineer", data_engineer_agent)
    workflow.add_node("admin_guide", admin_guide_agent)
    workflow.add_node("generate_answer", generate_final_answer)
    
    # ì‹œì‘ì ì—ì„œ ë¼ìš°íŒ… ë…¸ë“œë¡œ
    workflow.add_edge(START, "route")
    
    # ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€ (ë¼ìš°íŒ… ê²°ê³¼ì— ë”°ë¼)
    workflow.add_conditional_edges(
        "route",
        lambda x: x["route"],
        {
            "infra_engineer": "infra_engineer",
            "data_engineer": "data_engineer", 
            "admin_guide": "admin_guide"
        }
    )
    
    # ê° ì—ì´ì „íŠ¸ì—ì„œ ìµœì¢… ë‹µë³€ ìƒì„±ìœ¼ë¡œ
    workflow.add_edge("infra_engineer", "generate_answer")
    workflow.add_edge("data_engineer", "generate_answer")
    workflow.add_edge("admin_guide", "generate_answer")
    
    # ìµœì¢… ë‹µë³€ì—ì„œ ì¢…ë£Œ
    workflow.add_edge("generate_answer", END)
    
    # ê·¸ë˜í”„ ì»´íŒŒì¼
    return workflow.compile()

# ============================================================================
# 6. Gradio ì¸í„°í˜ì´ìŠ¤
# ============================================================================

def process_query(query: str) -> str:
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ë‹µë³€ì„ ë°˜í™˜"""
    try:
        # ì›Œí¬í”Œë¡œìš° ìƒì„±
        workflow = create_infrastructure_monitoring_workflow()
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        result = workflow.invoke({
            "user_query": query
        })
        
        return result.get("final_answer", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def create_gradio_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    with gr.Blocks(title="ë°ì´í„° ì¸í”„ë¼ ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ë°ì´í„° ì¸í”„ë¼ ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤")
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ ì§ˆë¬¸ ì…ë ¥")
                query_input = gr.Textbox(
                    label="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
                    placeholder="ì˜ˆ: í˜„ì¬ í´ëŸ¬ìŠ¤í„° ê°€ë™ìœ¨ ì•Œë ¤ì¤˜?",
                    lines=3
                )
                
                submit_btn = gr.Button("ğŸ” í™•ì¸", variant="primary")
                
                # ì˜ˆì‹œ ì§ˆë¬¸ë“¤
                gr.Markdown("### ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
                example_queries = [
                    "í˜„ì¬ í´ëŸ¬ìŠ¤í„° ìƒíƒœ ì•Œë ¤ì¤˜?",
                    "ë¦¬ì†ŒìŠ¤ë¥¼ ë§ì´ ì‚¬ìš©í•˜ëŠ” spark ì¡ 3ê°œ ì¶”ì¶œí•´ì¤˜?",
                    "ì´ ì¤‘ì—ì„œ ìƒìœ„ 2ê°œ spark ì¡ Kill í•´ì¤˜?",
                    "í´ëŸ¬ìŠ¤í„° ì‚¬ìš©ìœ¨ì´ ë–¨ì–´ì¡ŒëŠ”ì§€ í™•ì¸í•´ì¤˜?",
                    "í´ëŸ¬ìŠ¤í„° ì‚¬ìš©ìœ¨ì´ ë†’ì„ë•Œ ëŒ€ì‘ ê°€ì´ë“œë¥¼ ì°¾ì•„ì¤˜ "
                ]
                
                for example in example_queries:
                    gr.Button(example, size="sm").click(
                        lambda q=example: q,
                        outputs=query_input
                    )
            
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ¤– ë‹µë³€")
                result_output = gr.Markdown(
                    label="ë‹µë³€",
                    value="ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ë¶„ì„ì„ ì‹œì‘í•´ì£¼ì„¸ìš”."
                )
        
        # ì´ë²¤íŠ¸ ì—°ê²°
        submit_btn.click(
            fn=process_query,
            inputs=query_input,
            outputs=result_output
        )
        
        # Enter í‚¤ë¡œë„ ì œì¶œ ê°€ëŠ¥
        query_input.submit(
            fn=process_query,
            inputs=query_input,
            outputs=result_output
        )
    
    return interface

# ============================================================================
# 7. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë°ì´í„° ì¸í”„ë¼ ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±
    interface = create_gradio_interface()
    
    # ì„œë²„ ì‹œì‘
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True
    )

if __name__ == "__main__":
    main()