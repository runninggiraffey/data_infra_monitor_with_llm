#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM κΈ°λ° λ°μ΄ν„° μΈν”„λΌ λ¨λ‹ν„°λ§ μ„λΉ„μ¤
"""

import os
import json
import re
from datetime import datetime, timedelta
from typing import TypedDict, List, Dict, Optional, Literal, Annotated
from operator import add

# LangChain κ΄€λ ¨ imports
from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# LangGraph κ΄€λ ¨ imports
from langgraph.graph import StateGraph, START, END
from langgraph.types import Send
from langgraph.checkpoint.memory import InMemorySaver

# Gradio
import gradio as gr

# ν™κ²½ λ³€μ μ„¤μ •
from dotenv import load_dotenv
load_dotenv()

# # Langfuse μ¶”μ  ν™μ„±ν™”
# os.environ["LANGFUSE_PUBLIC_KEY"] = os.getenv("LANGFUSE_PUBLIC_KEY", "")
# os.environ["LANGFUSE_SECRET_KEY"] = os.getenv("LANGFUSE_SECRET_KEY", "")
# os.environ["LANGFUSE_HOST"] = os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com")
# os.environ["LANGCHAIN_TRACING_V2"] = "false"
# os.environ["LANGCHAIN_ENDPOINT"] = ""

# ============================================================================
# 1. μƒνƒ μ •μ
# ============================================================================

class InfrastructureState(TypedDict):
    """μΈν”„λΌ λ¨λ‹ν„°λ§ μƒνƒ"""
    # μ…λ ¥ λ°μ΄ν„°
    user_query: str                    # μ‚¬μ©μ μ§λ¬Έ
    alert_data: Optional[Dict]         # μ•λ λ°μ΄ν„°
    date_range: Optional[str]          # λ¶„μ„ κΈ°κ°„
    
    # λΌμ°ν… μ •λ³΄
    route: Optional[str]               # λΌμ°ν… κ²°κ³Ό
    
    # μμ§‘λ λ°μ΄ν„°
    hadoop_metrics: List[Dict]         # ν•λ‘΅ λ©”νΈλ¦­
    presto_metrics: List[Dict]         # Presto λ©”νΈλ¦­
    jenkins_metrics: List[Dict]        # Jenkins λ©”νΈλ¦­
    grafana_metrics: List[Dict]        # Grafana λ©”νΈλ¦­
    
    # λ¶„μ„ κ²°κ³Ό
    daily_report: Optional[str]        # μΌμΌ λ¦¬ν¬νΈ
    alert_analysis: Optional[str]      # μ•λ λ¶„μ„ κ²°κ³Ό
    spark_jobs: List[Dict]             # Spark μ΅ μ •λ³΄
    recommendations: List[str]         # κ¶μ¥μ‚¬ν•­
    
    # μ—μ΄μ „νΈ μ‘λ‹µ
    infra_engineer_response: Optional[str]
    data_engineer_response: Optional[str]
    admin_guide_response: Optional[str]
    
    # μµμΆ… κ²°κ³Ό
    final_answer: Optional[str]

# ============================================================================
# 2. λ„κµ¬ μ •μ (Tool Functions)
# ============================================================================

@tool
def get_cluster_metrics() -> Dict:
    """ν΄λ¬μ¤ν„° μ‹μ¤ν… λ¦¬μ†μ¤ μ‚¬μ©λ‰μ„ κ°€μ Έμµλ‹λ‹¤."""
    # λ°λ¨μ© κ°€μƒ λ°μ΄ν„°
    return {
        "timestamp": datetime.now().isoformat(),
        "cpu_usage": {
            "overall": 75.5,
            "node1": 82.3,
            "node2": 68.7,
            "node3": 79.1
        },
        "memory_usage": {
            "overall": 78.2,
            "node1": 85.6,
            "node2": 72.4,
            "node3": 76.5
        },
        "disk_usage": {
            "overall": 65.8,
            "node1": 71.2,
            "node2": 62.3,
            "node3": 63.9
        },
        "network_usage": {
            "overall": 45.3,
            "node1": 52.1,
            "node2": 41.8,
            "node3": 42.0
        }
    }

@tool
def get_spark_jobs() -> List[Dict]:
    """ν„μ¬ μ‹¤ν–‰ μ¤‘μΈ Spark μ΅ μ •λ³΄λ¥Ό κ°€μ Έμµλ‹λ‹¤."""
    # λ°λ¨μ© κ°€μƒ λ°μ΄ν„°
    return [
        {
            "job_id": "spark_001",
            "duration": "2h 15m",
            "cpu_usage": 85.2,
            "memory_usage": 78.5
        },
        {
            "job_id": "spark_002",
            "duration": "3h 45m",
            "cpu_usage": 92.1,
            "memory_usage": 89.3
        },
        {
            "job_id": "spark_003",
            "duration": "1h 30m",
            "cpu_usage": 45.7,
            "memory_usage": 52.1
        }
    ]

@tool
def kill_spark_job(job_id: str) -> Dict:
    """μ§€μ •λ Spark μ΅μ„ μΆ…λ£ν•©λ‹λ‹¤."""
    # λ°λ¨μ© κ°€μƒ μ‘λ‹µ
    return {
        "job_id": job_id,
        "action": "KILLED",
        "timestamp": datetime.now().isoformat(),
        "status": "SUCCESS",
        "message": f"Job {job_id} has been successfully terminated."
    }

# μ΄μ λ§¤λ‰΄μ–Ό λ²΅ν„° λ°μ΄ν„°λ² μ΄μ¤ μ΄κΈ°ν™”
def initialize_operation_manual_db():
    """μ΄μ λ§¤λ‰΄μ–Όμ„ Chroma λ²΅ν„° λ°μ΄ν„°λ² μ΄μ¤μ— λ΅λ“"""
    from langchain_community.document_loaders import TextLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    
    # ν…μ¤νΈ νμΌ λ΅λ“
    loader = TextLoader("data/operation_manual.txt", encoding="utf-8")
    documents = loader.load()
    
    # ν…μ¤νΈ λ¶„ν• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    splits = text_splitter.split_documents(documents)
    
    # Chroma λ²΅ν„° λ°μ΄ν„°λ² μ΄μ¤ μƒμ„±
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        collection_name="operation_manual",
        persist_directory="./chroma_db"
    )
    
    return vectorstore

# μ „μ—­ λ³€μλ΅ λ²΅ν„° λ°μ΄ν„°λ² μ΄μ¤ μ €μ¥
operation_manual_vectorstore = None

@tool
def search_operation_manual(query: str) -> List[Document]:
    """μ΄μ λ§¤λ‰΄μ–Όμ—μ„ κ΄€λ ¨ μ •λ³΄λ¥Ό λ²΅ν„° μ μ‚¬λ„ κ²€μƒ‰μΌλ΅ μ°Ύμµλ‹λ‹¤."""
    global operation_manual_vectorstore
    
    # λ²΅ν„° λ°μ΄ν„°λ² μ΄μ¤κ°€ μ—†μΌλ©΄ μ΄κΈ°ν™”
    if operation_manual_vectorstore is None:
        print("μ΄μ λ§¤λ‰΄μ–Ό λ²΅ν„° λ°μ΄ν„°λ² μ΄μ¤ μ΄κΈ°ν™” μ¤‘...")
        operation_manual_vectorstore = initialize_operation_manual_db()
    
    # μ μ‚¬λ„ κ²€μƒ‰ μν–‰
    docs = operation_manual_vectorstore.similarity_search(query, k=3)
    
    if docs:
        return docs
    else:
        return [Document(
            page_content="κ΄€λ ¨ μ΄μ λ§¤λ‰΄μ–Ό μ •λ³΄λ¥Ό μ°Ύμ„ μ μ—†μµλ‹λ‹¤.",
            metadata={"source": "yg_operation_manual.txt"}
        )]

# μ΄μ μ΄λ ¥ λ²΅ν„° λ°μ΄ν„°λ² μ΄μ¤ μ΄κΈ°ν™”
def initialize_operation_history_db():
    """μ΄μ μ΄λ ¥μ„ Chroma λ²΅ν„° λ°μ΄ν„°λ² μ΄μ¤μ— λ΅λ“"""
    from langchain_community.document_loaders import TextLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    
    # ν…μ¤νΈ νμΌ λ΅λ“
    loader = TextLoader("data/operation_history.txt", encoding="utf-8")
    documents = loader.load()
    
    # ν…μ¤νΈ λ¶„ν• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    splits = text_splitter.split_documents(documents)
    
    # Chroma λ²΅ν„° λ°μ΄ν„°λ² μ΄μ¤ μƒμ„±
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        collection_name="operation_history",
        persist_directory="./chroma_db"
    )
    
    return vectorstore

# μ „μ—­ λ³€μλ΅ λ²΅ν„° λ°μ΄ν„°λ² μ΄μ¤ μ €μ¥
operation_history_vectorstore = None

@tool
def search_operation_history(query: str) -> List[Document]:
    """μ΄μ μ΄λ ¥μ—μ„ κ΄€λ ¨ μ •λ³΄λ¥Ό λ²΅ν„° μ μ‚¬λ„ κ²€μƒ‰μΌλ΅ μ°Ύμµλ‹λ‹¤."""
    global operation_history_vectorstore
    
    # λ²΅ν„° λ°μ΄ν„°λ² μ΄μ¤κ°€ μ—†μΌλ©΄ μ΄κΈ°ν™”
    if operation_history_vectorstore is None:
        print("μ΄μ μ΄λ ¥ λ²΅ν„° λ°μ΄ν„°λ² μ΄μ¤ μ΄κΈ°ν™” μ¤‘...")
        operation_history_vectorstore = initialize_operation_history_db()
    
    # μ μ‚¬λ„ κ²€μƒ‰ μν–‰
    docs = operation_history_vectorstore.similarity_search(query, k=3)
    
    if docs:
        return docs
    else:
        return [Document(
            page_content="κ΄€λ ¨ μ΄μ μ΄λ ¥μ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤.",
            metadata={"source": "yg_operation_history.txt"}
        )]

@tool
def search_internet_info(query: str) -> List[Document]:
    """Tavilyλ¥Ό μ‚¬μ©ν•μ—¬ μΈν„°λ„·μ—μ„ κ΄€λ ¨ μ •λ³΄λ¥Ό κ²€μƒ‰ν•©λ‹λ‹¤."""
    try:
        from tavily import TavilyClient
        
        # Tavily ν΄λΌμ΄μ–ΈνΈ μ΄κΈ°ν™”
        tavily_api_key = os.getenv("TAVILY_API_KEY")
        if not tavily_api_key:
            error_msg = "TAVILY_API_KEYκ°€ μ„¤μ •λμ§€ μ•μ•μµλ‹λ‹¤. μΈν„°λ„· κ²€μƒ‰μ„ μν–‰ν•  μ μ—†μµλ‹λ‹¤."
            print(error_msg)
            return [Document(
                page_content=error_msg,
                metadata={"source": "error", "error_type": "missing_api_key"}
            )]
        
        client = TavilyClient(api_key=tavily_api_key)
        
        # κ²€μƒ‰ μΏΌλ¦¬ κ°μ„  (λ°μ΄ν„° μΈν”„λΌ κ΄€λ ¨ ν‚¤μ›λ“ μ¶”κ°€)
        enhanced_query = f"{query} data infrastructure monitoring best practices"
        
        # Tavily κ²€μƒ‰ μ‹¤ν–‰
        search_result = client.search(
            query=enhanced_query,
            search_depth="advanced",
            max_results=5,
            include_domains=["apache.org", "databricks.com", "cloudera.com", "hortonworks.com", "stackoverflow.com", "medium.com", "towardsdatascience.com"]
        )
        
        # κ²€μƒ‰ κ²°κ³Όλ¥Ό Document ν•νƒλ΅ λ³€ν™
        documents = []
        for result in search_result.get("results", []):
            content = result.get("content", "")
            url = result.get("url", "")
            title = result.get("title", "")
            
            if content:
                # μ λ©κ³Ό λ‚΄μ©μ„ κ²°ν•©
                full_content = f"μ λ©: {title}\n\nλ‚΄μ©: {content}"
                documents.append(Document(
                    page_content=full_content,
                    metadata={
                        "source": "tavily_search",
                        "url": url,
                        "title": title
                    }
                ))
        
        if documents:
            return documents
        else:
            error_msg = "Tavily κ²€μƒ‰ κ²°κ³Όκ°€ μ—†μµλ‹λ‹¤. μΈν„°λ„· κ²€μƒ‰μ„ μν–‰ν•  μ μ—†μµλ‹λ‹¤."
            print(error_msg)
            return [Document(
                page_content=error_msg,
                metadata={"source": "error", "error_type": "no_search_results"}
            )]
            
    except Exception as e:
        error_msg = f"Tavily κ²€μƒ‰ μ¤‘ μ¤λ¥ λ°μƒ: {e}. μΈν„°λ„· κ²€μƒ‰μ„ μν–‰ν•  μ μ—†μµλ‹λ‹¤."
        print(error_msg)
        return [Document(
            page_content=error_msg,
            metadata={"source": "error", "error_type": "search_error", "error_details": str(e)}
        )]

# λ„κµ¬ λ©λ΅
tools = [
    get_cluster_metrics,
    get_spark_jobs,
    kill_spark_job,
    search_operation_manual,
    search_operation_history,
    search_internet_info
]

# ============================================================================
# 3. LLM λ¨λΈ μ„¤μ •
# ============================================================================

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
llm_with_tools = llm.bind_tools(tools)

# ============================================================================
# 4. μ—μ΄μ „νΈ κµ¬ν„
# ============================================================================

def infra_engineer_agent(state: InfrastructureState) -> InfrastructureState:
    """μΈν”„λΌ μ—”μ§€λ‹μ–΄ μ—μ΄μ „νΈ - ν΄λ¬μ¤ν„° κ°€λ™μ¨ λ° μ‹μ¤ν… ν„ν™© λ‹΄λ‹Ή"""
    print("--- μΈν”„λΌ μ—”μ§€λ‹μ–΄ μ—μ΄μ „νΈ μ‹μ‘ ---")
    
    query = state["user_query"]
    
    # ν΄λ¬μ¤ν„° λ©”νΈλ¦­ κ°€μ Έμ¤κΈ° (Spark μ΅ λ΅μ§κ³Ό λ™μΌν•κ² λ³€κ²½)
    ai_msg = llm_with_tools.invoke(query)

    tool_call = ai_msg.tool_calls[0]
    tool_name = tool_call["name"]
    tool_args = tool_call["args"]

    for tool in tools:
        if tool_name == tool.name:
            metrics = tool.invoke(tool_args)
    
    # λ¶„μ„ ν”„λ΅¬ν”„νΈ
    analysis_prompt = ChatPromptTemplate.from_messages([
        ("system", """λ‹Ήμ‹ μ€ λ°μ΄ν„° μΈν”„λΌ μ—”μ§€λ‹μ–΄μ…λ‹λ‹¤. ν΄λ¬μ¤ν„°μ μ‹μ¤ν… λ¦¬μ†μ¤ μ‚¬μ© ν„ν™©μ„ λ¶„μ„ν•κ³  μ‚¬μ©μ μ§λ¬Έμ— λ‹µλ³€ν•΄μ£Όμ„Έμ”.
        
        λ¶„μ„ν•΄μ•Ό ν•  ν•­λ©:
        1. CPU μ‚¬μ©λ¥  (μ „μ²΄ λ° κ°λ³„ λ…Έλ“)
        2. λ©”λ¨λ¦¬ μ‚¬μ©λ¥  (μ „μ²΄ λ° κ°λ³„ λ…Έλ“)
        3. λ””μ¤ν¬ μ‚¬μ©λ¥  (μ „μ²΄ λ° κ°λ³„ λ…Έλ“)
        4. λ„¤νΈμ›ν¬ μ‚¬μ©λ¥  (μ „μ²΄ λ° κ°λ³„ λ…Έλ“)
        
        λ‹µλ³€ ν•μ‹:
        - ν„μ¬ ν΄λ¬μ¤ν„° μƒνƒ μ”μ•½
        - κ° λ…Έλ“λ³„ μƒμ„Έ ν„ν™©
        - μ£Όμκ°€ ν•„μ”ν• λ¶€λ¶„
        - κ¶μ¥μ‚¬ν•­"""),
        ("human", "μ‚¬μ©μ μ§λ¬Έ: {query}\n\nν΄λ¬μ¤ν„° λ©”νΈλ¦­: {metrics}")
    ])
    
    response = llm.invoke(analysis_prompt.format(
        query=query,
        metrics=json.dumps(metrics, indent=2, ensure_ascii=False)
    ))
    
    return {
        "hadoop_metrics": [metrics],
        "infra_engineer_response": response.content,
        "data_engineer_response": state.get("data_engineer_response", ""),
        "admin_guide_response": state.get("admin_guide_response", "")
    }

def data_engineer_agent(state: InfrastructureState) -> InfrastructureState:
    """λ°μ΄ν„° μ—”μ§€λ‹μ–΄ μ—μ΄μ „νΈ - Spark μ΅ κ΄€λ¦¬ λ‹΄λ‹Ή"""
    print("--- λ°μ΄ν„° μ—”μ§€λ‹μ–΄ μ—μ΄μ „νΈ μ‹μ‘ ---")
    
    query = state["user_query"]
    
    # Spark μ΅ μ •λ³΄ κ°€μ Έμ¤κΈ°
    ai_msg = llm_with_tools.invoke(query)

    tool_call = ai_msg.tool_calls[0]
    tool_name = tool_call["name"]
    tool_args = tool_call["args"]

    for tool in tools:
        if tool_name == tool.name:
            spark_jobs = tool.invoke(tool_args)
    
    # μƒμ„ 3κ° μ΅ μ¶”μ¶ (CPU μ‚¬μ©λ¥  κΈ°μ¤€)
    top_jobs = sorted(spark_jobs, key=lambda x: x["cpu_usage"], reverse=True)[:3]
    
    # λ¶„μ„ ν”„λ΅¬ν”„νΈ
    analysis_prompt = ChatPromptTemplate.from_messages([
        ("system", """λ‹Ήμ‹ μ€ λ°μ΄ν„° μ—”μ§€λ‹μ–΄μ…λ‹λ‹¤. Spark μ΅ ν„ν™©μ„ λ¶„μ„ν•κ³  μ‚¬μ©μ μ§λ¬Έμ— λ‹µλ³€ν•΄μ£Όμ„Έμ”.
        
        λ¶„μ„ν•΄μ•Ό ν•  ν•­λ©:
        1. ν„μ¬ μ‹¤ν–‰ μ¤‘μΈ Spark μ΅ λ©λ΅
        2. λ¦¬μ†μ¤ μ‚¬μ©λ‰μ΄ λ†’μ€ μ΅ μ‹λ³„
        3. μν–‰ μ‹κ°„μ΄ κΈ΄ μ΅ ν™•μΈ
        4. μ΅ μΆ…λ£ κ¶μ¥μ‚¬ν•­
        
        λ‹µλ³€ ν•μ‹:
        - Spark μ΅ ν„ν™© μ”μ•½
        - μƒμ„ 3κ° μ΅ μƒμ„Έ μ •λ³΄
        - λ¦¬μ†μ¤ μ‚¬μ©λ‰ λ¶„μ„
        - μ΅ κ΄€λ¦¬ κ¶μ¥μ‚¬ν•­"""),
        ("human", "μ‚¬μ©μ μ§λ¬Έ: {query}\n\nSpark μ΅ μ •λ³΄: {spark_jobs}")
    ])
    
    response = llm.invoke(analysis_prompt.format(
        query=query,
        spark_jobs=json.dumps(top_jobs, indent=2, ensure_ascii=False)
    ))
    
    return {
        "spark_jobs": top_jobs,
        "data_engineer_response": response.content,
        "infra_engineer_response": state.get("infra_engineer_response", ""),
        "admin_guide_response": state.get("admin_guide_response", "")
    }

def admin_guide_agent(state: InfrastructureState) -> InfrastructureState:
    """κ΄€λ¦¬μ κ°€μ΄λ“ μ—μ΄μ „νΈ - μ΄μ λ§¤λ‰΄μ–Ό λ° μ΄λ ¥ κ²€μƒ‰ λ‹΄λ‹Ή"""
    print("--- κ΄€λ¦¬μ κ°€μ΄λ“ μ—μ΄μ „νΈ μ‹μ‘ ---")
    
    query = state["user_query"]
    
    # μ΄μ λ§¤λ‰΄μ–Ό, μ΄μ μ΄λ ¥, μΈν„°λ„· μ •λ³΄ κ²€μƒ‰μ„ κ°κ° tool νΈμ¶ λ°©μ‹μΌλ΅ μ²λ¦¬ (ai_msg μ¤‘λ³µ λ°©μ§€)
    manual_tool_call = llm_with_tools.invoke(f"μ΄μ λ§¤λ‰΄μ–Όμ—μ„ λ‹µμ„ μ°Ύμ•„μ¤: {query}")
    manual_tool_name = manual_tool_call.tool_calls[0]["name"]
    manual_tool_args = manual_tool_call.tool_calls[0]["args"]
    manual_docs = None
    for tool in tools:
        if manual_tool_name == tool.name:
            manual_docs = tool.invoke(manual_tool_args)

    history_tool_call = llm_with_tools.invoke(f"μ΄μ μ΄λ ¥μ—μ„ λ‹µμ„ μ°Ύμ•„μ¤: {query}")
    history_tool_name = history_tool_call.tool_calls[0]["name"]
    history_tool_args = history_tool_call.tool_calls[0]["args"]
    history_docs = None
    for tool in tools:
        if history_tool_name == tool.name:
            history_docs = tool.invoke(history_tool_args)

    internet_tool_call = llm_with_tools.invoke(f"μΈν„°λ„·μ—μ„ λ‹µμ„ μ°Ύμ•„μ¤: {query}")
    internet_tool_name = internet_tool_call.tool_calls[0]["name"]
    internet_tool_args = internet_tool_call.tool_calls[0]["args"]
    internet_docs = None
    for tool in tools:
        if internet_tool_name == tool.name:
            internet_docs = tool.invoke(internet_tool_args)
    
    # λ¶„μ„ ν”„λ΅¬ν”„νΈ
    analysis_prompt = ChatPromptTemplate.from_messages([
        ("system", """λ‹Ήμ‹ μ€ λ°μ΄ν„° μΈν”„λΌ κ΄€λ¦¬μμ…λ‹λ‹¤. μ΄μ λ§¤λ‰΄μ–Ό, μ΄μ „ μ΄μ μ΄λ ¥, μΈν„°λ„· μ •λ³΄λ¥Ό μΆ…ν•©ν•μ—¬ μ‚¬μ©μ μ§λ¬Έμ— λ‹µλ³€ν•΄μ£Όμ„Έμ”.
        
        μ°Έκ³  μλ£:
        1. μ΄μ λ§¤λ‰΄μ–Ό: ν‘μ¤€ μ΄μ μ μ°¨ λ° κ°€μ΄λ“λΌμΈ
        2. μ΄μ μ΄λ ¥: μ΄μ „μ— λ°μƒν• μ μ‚¬ μƒν™©κ³Ό ν•΄κ²° λ°©λ²•
        3. μΈν„°λ„· μ •λ³΄: μµμ‹  κΈ°μ  μ •λ³΄ λ° λ¨λ²” μ‚¬λ΅€
        
        λ‹µλ³€ ν•μ‹:
        - μ΄μ λ§¤λ‰΄μ–Ό κΈ°λ° ν‘μ¤€ μ μ°¨
        - μ΄μ „ μ‚¬λ΅€ λ¶„μ„
        - μµμ‹  μ •λ³΄ λ° κ¶μ¥μ‚¬ν•­
        - μΆ…ν•©μ μΈ ν•΄κ²° λ°©μ•"""),
        ("human", """μ‚¬μ©μ μ§λ¬Έ: {query}

μ΄μ λ§¤λ‰΄μ–Ό:
{manual_docs}

μ΄μ μ΄λ ¥:
{history_docs}

μΈν„°λ„· μ •λ³΄:
{internet_docs}""")
    ])
    
    response = llm.invoke(analysis_prompt.format(
        query=query,
        manual_docs="\n".join([doc.page_content for doc in manual_docs]),
        history_docs="\n".join([doc.page_content for doc in history_docs]),
        internet_docs="\n".join([doc.page_content for doc in internet_docs])
    ))
    
    return {
        "admin_guide_response": response.content,
        "infra_engineer_response": state.get("infra_engineer_response", ""),
        "data_engineer_response": state.get("data_engineer_response", "")
    }

def route_query(state: InfrastructureState) -> InfrastructureState:
    """μ‚¬μ©μ μ§λ¬Έμ„ λ¶„μ„ν•μ—¬ μ μ ν• μ—μ΄μ „νΈλ΅ λΌμ°ν…"""
    print("--- μ§λ¬Έ λΌμ°ν… λ¶„μ„ ---")
    
    query = state["user_query"].lower()
    
    # μΈν”„λΌ κ΄€λ ¨ ν‚¤μ›λ“
    infra_keywords = [
        "ν΄λ¬μ¤ν„°", "cpu", "λ©”λ¨λ¦¬", "λ””μ¤ν¬", "λ„¤νΈμ›ν¬", "λ¦¬μ†μ¤", "κ°€λ™μ¨", "μ‚¬μ©λ¥ ",
        "μ‹μ¤ν…", "λ…Έλ“", "μ„λ²„", "μ„±λ¥", "λ¶€ν•", "λ¶€μ΅±", "λ†’μ", "λ‚®μ",
        "λ¨λ‹ν„°λ§", "λ©”νΈλ¦­", "ν„ν™©", "μƒνƒ", "ν™•μΈ", "μ κ²€"
    ]
    
    # λ°μ΄ν„°/λ°μ΄ν„°ν”λ«νΌ κ΄€λ ¨ ν‚¤μ›λ“
    data_keywords = [
        "spark", "hadoop", "hdfs", "yarn", "presto", "hive", "kafka", "flink",
        "μ΅", "job", "νμ΄ν”„λΌμΈ", "pipeline", "etl", "λ°μ΄ν„°", "μ²λ¦¬",
        "kill", "μΆ…λ£", "μ‹¤ν–‰", "μ¤μΌ€μ¤„", "ν", "λ©”λ¨λ¦¬", "cpu μ‚¬μ©λ¥ "
    ]
    
    # μ΄μ λ§¤λ‰΄μ–Ό/μ΄λ ¥ κ΄€λ ¨ ν‚¤μ›λ“
    admin_keywords = [
        "λ§¤λ‰΄μ–Ό", "μ΄μ", "μ μ°¨", "κ°€μ΄λ“", "λ°©λ²•", "μ΅°μΉ", "λ€μ‘", "ν•΄κ²°",
        "μ΄λ ¥", "κ²½ν—", "μ‚¬λ΅€", "μ „λ΅€", "μ΄μ „", "κ³Όκ±°", "ν•™μµμ ",
        "μ–΄λ–»κ²", "λ¬΄μ—‡μ„", "μ™", "μ–Έμ ", "μ–΄λ””μ„", "λ„κ°€"
    ]
    
    # ν‚¤μ›λ“ λ§¤μΉ­ μ μ κ³„μ‚°
    infra_score = sum(1 for keyword in infra_keywords if keyword in query)
    data_score = sum(1 for keyword in data_keywords if keyword in query)
    admin_score = sum(1 for keyword in admin_keywords if keyword in query)
    
    print(f"λΌμ°ν… μ μ - μΈν”„λΌ: {infra_score}, λ°μ΄ν„°: {data_score}, κ΄€λ¦¬μ: {admin_score}")
    
    # κ°€μ¥ λ†’μ€ μ μμ μ—μ΄μ „νΈλ΅ λΌμ°ν…
    if data_score > infra_score and data_score > admin_score:
        print("β†’ λ°μ΄ν„° μ—”μ§€λ‹μ–΄ μ—μ΄μ „νΈλ΅ λΌμ°ν…")
        return {"route": "data_engineer"}
    elif admin_score > infra_score and admin_score > data_score:
        print("β†’ κ΄€λ¦¬μ κ°€μ΄λ“ μ—μ΄μ „νΈλ΅ λΌμ°ν…")
        return {"route": "admin_guide"}
    else:
        print("β†’ μΈν”„λΌ μ—”μ§€λ‹μ–΄ μ—μ΄μ „νΈλ΅ λΌμ°ν…")
        return {"route": "infra_engineer"}

def generate_final_answer(state: InfrastructureState) -> InfrastructureState:
    """μµμΆ… λ‹µλ³€ μƒμ„±"""
    print("--- μµμΆ… λ‹µλ³€ μƒμ„± ---")
    
    query = state["user_query"]
    infra_response = state.get("infra_engineer_response", "")
    data_response = state.get("data_engineer_response", "")
    admin_response = state.get("admin_guide_response", "")
    
    # μµμΆ… λ‹µλ³€ μƒμ„± ν”„λ΅¬ν”„νΈ
    final_prompt = ChatPromptTemplate.from_messages([
        ("system", """λ‹Ήμ‹ μ€ λ°μ΄ν„° μΈν”„λΌ λ¨λ‹ν„°λ§ μ „λ¬Έκ°€μ…λ‹λ‹¤. μ²λ¦¬λ μ—μ΄μ „νΈμ μ‘λ‹µμ„ λ°”νƒ•μΌλ΅ μ‚¬μ©μμ—κ² λ…ν™•ν•κ³  μ‹¤μ©μ μΈ λ‹µλ³€μ„ μ κ³µν•΄μ£Όμ„Έμ”.
        
        λ‹µλ³€ κµ¬μ΅°:
        1. ν„μ¬ μƒν™© μ”μ•½
        2. μƒμ„Έ λ¶„μ„ κ²°κ³Ό
        3. κµ¬μ²΄μ μΈ μ΅°μΉ λ°©μ•
        4. μλ°© λ° λ¨λ‹ν„°λ§ κ¶μ¥μ‚¬ν•­
        
        κ° μ—μ΄μ „νΈμ μ „λ¬Έ λ¶„μ•Ό:
        - μΈν”„λΌ μ—”μ§€λ‹μ–΄: μ‹μ¤ν… λ¦¬μ†μ¤ ν„ν™© λ° ν΄λ¬μ¤ν„° μƒνƒ
        - λ°μ΄ν„° μ—”μ§€λ‹μ–΄: Spark μ΅ κ΄€λ¦¬ λ° μµμ ν™”
        - κ΄€λ¦¬μ κ°€μ΄λ“: μ΄μ μ μ°¨ λ° μ΄μ „ μ‚¬λ΅€"""),
        ("human", """μ‚¬μ©μ μ§λ¬Έ: {query}

μΈν”„λΌ μ—”μ§€λ‹μ–΄ μ‘λ‹µ:
{infra_response}

λ°μ΄ν„° μ—”μ§€λ‹μ–΄ μ‘λ‹µ:
{data_response}

κ΄€λ¦¬μ κ°€μ΄λ“ μ‘λ‹µ:
{admin_response}

μ„ μ •λ³΄λ¥Ό μΆ…ν•©ν•μ—¬ μµμΆ… λ‹µλ³€μ„ μ‘μ„±ν•΄μ£Όμ„Έμ”.""")
    ])
    
    response = llm.invoke(final_prompt.format(
        query=query,
        infra_response=infra_response,
        data_response=data_response,
        admin_response=admin_response
    ))
    
    return {
        "final_answer": response.content
    }

# ============================================================================
# 5. LangGraph μ›ν¬ν”λ΅μ° κµ¬μ„±
# ============================================================================

def create_infrastructure_monitoring_workflow():
    """μΈν”„λΌ λ¨λ‹ν„°λ§ μ›ν¬ν”λ΅μ° μƒμ„±"""
    
    # κ·Έλν”„ μƒμ„±
    workflow = StateGraph(InfrastructureState)
    
    # λ…Έλ“ μ¶”κ°€
    workflow.add_node("route", route_query)
    workflow.add_node("infra_engineer", infra_engineer_agent)
    workflow.add_node("data_engineer", data_engineer_agent)
    workflow.add_node("admin_guide", admin_guide_agent)
    workflow.add_node("generate_answer", generate_final_answer)
    
    # μ‹μ‘μ μ—μ„ λΌμ°ν… λ…Έλ“λ΅
    workflow.add_edge(START, "route")
    
    # μ΅°κ±΄λ¶€ μ—£μ§€ μ¶”κ°€ (λΌμ°ν… κ²°κ³Όμ— λ”°λΌ)
    workflow.add_conditional_edges(
        "route",
        lambda x: x["route"],
        {
            "infra_engineer": "infra_engineer",
            "data_engineer": "data_engineer", 
            "admin_guide": "admin_guide"
        }
    )
    
    # κ° μ—μ΄μ „νΈμ—μ„ μµμΆ… λ‹µλ³€ μƒμ„±μΌλ΅
    workflow.add_edge("infra_engineer", "generate_answer")
    workflow.add_edge("data_engineer", "generate_answer")
    workflow.add_edge("admin_guide", "generate_answer")
    
    # μµμΆ… λ‹µλ³€μ—μ„ μΆ…λ£
    workflow.add_edge("generate_answer", END)
    
    # κ·Έλν”„ μ»΄νμΌ
    return workflow.compile()

# ============================================================================
# 6. Gradio μΈν„°νμ΄μ¤
# ============================================================================

def process_query(query: str) -> str:
    """μ‚¬μ©μ μ§λ¬Έμ„ μ²λ¦¬ν•κ³  λ‹µλ³€μ„ λ°ν™"""
    try:
        # μ›ν¬ν”λ΅μ° μƒμ„±
        workflow = create_infrastructure_monitoring_workflow()
        
        # μ›ν¬ν”λ΅μ° μ‹¤ν–‰
        result = workflow.invoke({
            "user_query": query
        })
        
        return result.get("final_answer", "λ‹µλ³€μ„ μƒμ„±ν•  μ μ—†μµλ‹λ‹¤.")
        
    except Exception as e:
        return f"μ¤λ¥κ°€ λ°μƒν–μµλ‹λ‹¤: {str(e)}"

def create_gradio_interface():
    """Gradio μΈν„°νμ΄μ¤ μƒμ„±"""
    
    with gr.Blocks(title="λ°μ΄ν„° μΈν”„λΌ λ¨λ‹ν„°λ§ μ„λΉ„μ¤", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# π—οΈ LLM κΈ°λ° λ°μ΄ν„° μΈν”„λΌ λ¨λ‹ν„°λ§ μ„λΉ„μ¤")
        gr.Markdown("ν΄λ¬μ¤ν„° CPU μ‚¬μ©μ¨μ΄ λ†’λ‹¤λ” μ•λμ„ λ°›μ€ ν›„ μ±—λ΄‡μ„ ν†µν•΄ μΈν”„λΌ μƒν™©μ„ λ¶„μ„ν•κ³  μ΅°μΉν•  μ μμµλ‹λ‹¤.")
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### π“ μ§λ¬Έ μ…λ ¥")
                query_input = gr.Textbox(
                    label="μ§λ¬Έμ„ μ…λ ¥ν•μ„Έμ”",
                    placeholder="μ: ν„μ¬ ν΄λ¬μ¤ν„° κ°€λ™μ¨ μ•λ ¤μ¤?",
                    lines=3
                )
                
                submit_btn = gr.Button("π” λ¶„μ„ μ‹μ‘", variant="primary")
                
                # μμ‹ μ§λ¬Έλ“¤
                gr.Markdown("### π’΅ μμ‹ μ§λ¬Έ")
                example_queries = [
                    "ν„μ¬ ν΄λ¬μ¤ν„° κ°€λ™μ¨ μ•λ ¤μ¤?",
                    "ν΄λ¬μ¤ν„° κ°€λ™μ¨μ΄ λ†’μ„λ• μ–΄λ–»κ² ν•΄μ•Όλ?",
                    "λ¦¬μ†μ¤λ¥Ό λ§μ΄ μ‚¬μ©ν•λ” spark μ΅ 3κ° μ¶”μ¶ν•΄μ¤?",
                    "μ΄ μ¤‘μ—μ„ μƒμ„ 2κ° spark μ΅ Kill ν•΄μ¤?",
                    "ν΄λ¬μ¤ν„° μ‚¬μ©μ¨μ΄ λ–¨μ–΄μ΅λ”μ§€ ν™•μΈν•΄μ¤?",
                    "μ΄μ λ§¤λ‰΄μ–Όμ—μ„ μ¥μ•  μ΅°μΉ λ°©λ²• μ°Ύμ•„μ¤.",
                    "μ΄μ „μ— λΉ„μ·ν• μ¥μ• κ°€ μμ—λ”μ§€ μ΄μ μ΄λ ¥μ—μ„ κ²€μƒ‰ν•΄μ¤."
                ]
                
                for example in example_queries:
                    gr.Button(example, size="sm").click(
                        lambda q=example: q,
                        outputs=query_input
                    )
            
            with gr.Column(scale=2):
                gr.Markdown("### π¤– λ¶„μ„ κ²°κ³Ό")
                result_output = gr.Markdown(
                    label="λ¶„μ„ κ²°κ³Ό",
                    value="μ§λ¬Έμ„ μ…λ ¥ν•κ³  λ¶„μ„μ„ μ‹μ‘ν•΄μ£Όμ„Έμ”."
                )
        
        # μ΄λ²¤νΈ μ—°κ²°
        submit_btn.click(
            fn=process_query,
            inputs=query_input,
            outputs=result_output
        )
        
        # Enter ν‚¤λ΅λ„ μ μ¶ κ°€λ¥
        query_input.submit(
            fn=process_query,
            inputs=query_input,
            outputs=result_output
        )
    
    return interface

# ============================================================================
# 7. λ©”μΈ μ‹¤ν–‰ ν•¨μ
# ============================================================================

def main():
    """λ©”μΈ μ‹¤ν–‰ ν•¨μ"""
    print("π€ λ°μ΄ν„° μΈν”„λΌ λ¨λ‹ν„°λ§ μ„λΉ„μ¤λ¥Ό μ‹μ‘ν•©λ‹λ‹¤...")
    
    # Gradio μΈν„°νμ΄μ¤ μƒμ„±
    interface = create_gradio_interface()
    
    # μ„λ²„ μ‹μ‘
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True
    )

if __name__ == "__main__":
    main()